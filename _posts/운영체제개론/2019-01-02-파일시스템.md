---
title: "파일 시스템"
read_time: false
share: false
toc: true
author_profile: false
classes: wide


categories:
  - OperatingSystem
tags:
  - KOCW
  - 반효경교수님
  - OS
---

## File system
### File
이름을 통해 접근하는 단위. 비 휘발성 보조기억장치에 저장하게 된다.  
리눅스 등의 운영체제에서 File은 꼭 데이터 저장 목적이 아닌 여러 장치를 논리적 단위로 관리하기 위한 목적으로 사용하기도 한다(Device special file).
<br>

#### File과 관련된 연산들
File create, file delete, file read & write, file repositioning, file open & close 가 있다.  
file repositioning(lseek) - file은 크기가 크기 때문에 어느 위치를 읽거나 쓰는지 가리키는 포인터가 필요한데 이런 접근위치를 수정해주는 연산이다.  
file open & close – read와 write를 하기 전과 후 의무적으로 수행하는 연산, 따로 정의돼 있는 이유는 disk에서 메모리로 내용을 옮기는 것이 아닌 file의 meta data를 메모리로 옮기는 작업이다. 아래에 자세히 설명.  

Directory 또한 하나의 파일로써 file의 특성들을 갖고 있다. 단 파일내용이 다른 파일들의 meta data 중 일부(혹은 전부)를 내용으로 저장하고 있는 특별한 파일이다.
디렉토리를 위한 연산도 file에서 사용하는 연산처럼 여러가지다.  
Search file, create file, delete file, list a directory, rename file, traverse the file system(파일 시스템 전체 탐생) 가 있다.  
<br>

#### File Attribute(metadata)
file을 수정하는 것이 목적이 아니라 관리하기 위한 목적의 정보. 파일 이름, 파일 접근권한, 형식, 시간, 유형, 디스크상 저장 위치, 크기 등 관리를 목적으로 만들어진 정보
<br>
<br>

***

### File System 개요
OS의 파일 관리 소프트웨어. 파일 자체 내용 뿐 아니라 파일 metadata도 같이 관리한다.  
파일 저장을 보통 1차원 적으로 저장하지 않고 관리와 편의성을 위해 디렉토리를 두고 계층적으로 저장한다. 파일 저장방식, 파일 관리, 파일 보호 등을 관리하는 소프트웨어이다.  

이러한 파일 시스템은 결국 디스크에 저장되는데 디스크 또한 메모리처럼 논리 디스크와 물리 디스크로 나뉜다. 운영체제가 보는것은 논리 디스크로 파티션으로 불린다.  
논리 디스크는 하나의 물리디스크를 여러 개 논리디스크로 구성할 수도 있고 여러 개의 물리디스크를 하나의 논리디스크로 구성할 수 도 있다.  
디스크의 용도는 크게 file system용도, swap area 용도 2가지로 사용한다. system이 깔려 사용되거나 메모리 swapping 등의 용도로 사용된다.  
<br>

#### File system - File open 과정
<u>File open은 file의 meta data를 메모리에 올려 놓는 시스템콜이다.</u> meta data 중엔 파일내용의 저장 위치 등 여러 정보가 있기 때문에 meta data부터 올리고 file content를 올린다.  

memory에는 PCB, 각 PCB안에는 메모리에 올라온 파일 meta data 주소가 저장된 table(per-process file descriptor table), 모든 프로세스에서 open된 파일들을 관리하기 위한 system-wide open file table이 올라가 있다.  
disk에는 각종 파일의 meta data와 내용(content)이 있다.  
![os_12_1]({{ "/assets/OS/OS_12_1.png" | absolute_url }}){: width="500" ){: .center}  

프로세스가 /a/b file을 open하는시스템 콜을 발생하면 b의 meta data를 메모리로 올려야 할 것이다. 이때 b의 meta data가 어디 있는지 모르니 디스크에서 찾아야 하는데 이 과정이 위의 그림과 같다.  

이미 알려져 있는 root directory('/')의 meta data를 open하고 실제 content위치로 이동한다. 루트 디렉토리 안에는 a의 meta data가 있을 것이고 이를 반복해서 디렉토리a와 파일b를 open한다.   

각 프로세스 마다 open한 파일에 대한 meta data 주소를 갖고 있는 배열(per-process file descriptor table)이 PCB에 정의되어 있다.  

OS가 b의 meta data를 메모리에 올리고 주소 값을 배열에 넣어 주었고 해당 배열의 index(file descriptor)를 open함수의 반환 값으로 사용자 프로세스한테 전달한다.  

meta data는 메모리에 올라가 있고 배열안에 meta data 주소도 있기 때문에 fd를 사용해서 파일 내용에 접근하면 된다(다시 접근할 때 루트 디렉토리부터 가지 않아도 됨).  

open후에 사용자 프로세스는 read함수와 fd를 사용해서 시스템 콜을 하였고 OS는 fd가지고 meta data 주소를 얻고 파일b의 content로 접근한다.  
OS는 프로세스가 요청한 위치의 요청한 용량만큼 내용을 읽고 메모리로 읽어 프로세스에게 전달하면 된다.  

읽은 메모리는 바로 사용자 프로세스에게 주는 것이 아니라 OS의 메모리 공간 일부에 copy하고 넘겨준다.  
만약 해당 프로세스 혹은 다른 프로세스가 동일한 파일의 동일한 위치의 read시스템콜을 요청하면 바로 읽어 놓은 내용을 전달하면 된다. 이를 buffer cashing이라 한다.  

buffer cashing 시스템에서 관련 연산은 모두 시스템 콜이라 OS가 먼저 CPU를 받기 때문에 전에 배웠던 LFU, LRU알고리즘을 버퍼 캐시에 입력된 데이터에 써먹을 수 있다(버퍼캐싱이 꽉 차면 어느거 부터 지울지 결정 가능). paging에서 clock알고리즘을 쓴 것과는 대조된다.
<br>

##### 추가설명
여기서 per-process file descriptor table과 system-wide open file table 두개를 설명했는데 구현에 따라서 table종류가 여러가지로 될 수 있다.  
실제 OS에선 추가적으로 필요한 meta data가 있다. 현재 프로세스가 파일의 어느 위치를 접근하고 있는지를 표시하는 offset을 관리할 meta data이다.  

이는 프로세스마다 별도로 가지고 있기 때문에 위의 Open file table을 나누어서 전체 프로세스의 meta data table(system-wide open file table)과 각 프로세스가 가리키고 있는 offset을 관리하는 table 2개를 같이 두는 것이 일반적이다. 
<br>
<br>

***

### File Protection
메모리에 대한 접근보호는 프로세스마다 메모리를 별도로 갖고 있어서 남이 건들일 일이 없기 때문에 page마다 rw를 모두 할 수 있나 없나만 구분했다(보안에 신경쓸게 별로 없음).  
하지만 파일의 경우엔 한 파일을 여러 프로세스(사용자)가 같이 사용할 수 있기 때문에 각 파일에 대해 누구에게 어떤 유형의 접근을 허락할 것인지 제어해야 한다.  

접근권한을 제어하는 방법은 크게 3가지로 나뉜다.  

**1. Access control matrix** 
![os_12_2]({{ "/assets/OS/OS_12_2.png" | absolute_url }})   
행렬을 사용해 사용자들과 파일에 어떤 권한이 있는지 모두 검사하는 것이다.  
파일의 수가 사용자수에 비해 압도적으로 많기 때문에 희소행렬(0이 많은)이 될 것이라 비효율적이다.  

그래서 Access control matrix에 linked list 자료구조를 사용해 효율적으로 구현할 수 있다.  
파일마다 리스트를 만들어 해당 파일에 대해서 사용권한이 있는 사용자를 줄 세우는 방법과 사용자마다 list를 만들어 파일을 줄 세우는 방법이 있다.  
사용자 중심 리스트를 capability 리스트라고 한다. 접근권한이 없는 사용자는 list에 낄 필요가 없고 접근권한이 없는 파일은 사용자의 리스트에 낄 필요가 없어 공간 낭비가 적다.  

리스트 방식도 오버헤드가 크기 때문에 일반적인 OS는 Grouping을 사용해서 접근권한을 다룬다.  
<br>

**2. Grouping**
우리가 흔히 알고 있는 <u>owner, group, public</u> 세 그룹으로 나누고 rwx-rwx-rwx 9bit를 통해 파일 접근권한을 관리하는 방식이다.  

**3. Password**
파일마다 password를 두는 방법. 읽기 쓰기 실행 별로 비밀번호를 따로 두어야 하고 관리적으로 사용하기 힘든 방법으로 거의 사용되지 않는다.  
<br>
<br>

***

### File System의 Mounting
하나의 물리적 디스크는 partitioning을 통해 여러 개의 논리적 디스크로 나눌 수 있다(혹은 그반대). 그리고 각각의 논리적 디스크에는 파일 시스템을 설치해 사용할 수 있다.  

![os_12_3]({{ "/assets/OS/OS_12_3.png" | absolute_url }}){: width="400" }{: .center}  
서로 다른 파티션의 파일 시스템을 같이 사용하기 위해선 그림처럼 기존 루트 파일 시스템의 특정 디렉토리 이름에다 해당 파일 시스템을 마운트를 해주면 사용 가능하다.
<br>
<br>

***

### 파일 시스템의 파일 접근 방법
**1.순차접근(Sequential access)**
카세트나 비디오 같은 테이프 매체, 중간부터 보려 해도 처음부터 돌린 다음에 접근하는 방식을 순차접근이라 한다. 
<br>

**2.직접접근(Direct Access, Random Access: 임의접근)**
레코드판, CD, HardDisk 같은 매체, 특정 위치를 접근한 다음에 바로 다음 위치로 접근하는 방식은 직접접근이라 한다.  
직접접근이 가능한 매체라 하더라도 <u>데이터 관리를 어떻게 하는지에 따라 순차접근으로 접근해야 할 수도 있다.</u>
<br>
<br>

***

### File system implement (파일 시스템 구현)

Disk에서 파일 저장 방법은 3가지로 나뉜다.
<br>

#### Contiguous Allocation (연속 할당방법)
![os_12_4]({{ "/assets/OS/OS_12_4.png" | absolute_url }})   
디렉토리는 파일의 내용이다, 아래 파일들의 메타데이터를 가지고 있다.  
위 그림처럼 파일 이름, 파일 시작위치, 크기 등을 내용으로 가지고 있다.  
예로 Count파일은 0부터 2까지 공간 차지, tr파일은 14부터 17까지 내용을 저장하고 있다.
 
디렉토리 안 메타데이터들 내용대로라면 Disk의 저장 공간은 아래 그림처럼 된다.
![os_12_5]({{ "/assets/OS/OS_12_5.png" | absolute_url }})   
파일시스템에서 동일한 크기의 저장 단위(네모)를 논리적 블록(섹터)이라 한다. 파일의 크기는 다 제각각 이기 때문에 섹터 단위로 데이터를 나누어 저장한다.  
연속 저장 이기 때문에 근접한 블록끼리 연속으로 구성되어 파일 저장이 된다.  
<br>

**Contiguous Allocation 단점**
1. External fragmentation(외부조각) 발생  
파일 크기가 동일하지 않기 때문에 외부조각이 생김으로 비어 있는 공간이 있음에도 활용하지 못하는 비효율성이 생긴다.  
<br>

2. File grow (파일크기 증가)의 제한  
수정을 통해 파일크기가 커질 수 있는데 연속할당 규칙에 따라 크기증가에 제한이 생긴다. 커질 것을 대비해 빈 공간을 배치해 놓는 방법이 있지만 이럴 경우 내부조각이 발생하고(배치해 놓고 사용하지 않는 블록이 생김) 파일 증가가 확실히 해결되는 것도 아니다.
<br>

**Contiguous Allocation 장점**
1. Fast I/O  
속도가 빠르다. Disk에서 I/O소요시간은 대부분 head가 이동하는 시간인데 한번만 이동하면 연속된 데이터를 통째로 읽고 쓸 수 있기 때문에 빠르다. 공간효율보단 속도효율이 중요한 swapping용도와 데드라인이 있는 Realtime file용으로 효율적이다.  
<br>

2. Direct access (Random access) 가능  
직접접근 가능하다. 연속할당시스템에서 mail이라는 파일의 4번째 블록부터 보고싶다 할 경우  시작위치에서 4번째 뒤의 블록으로 가면 된다.
<br>
<br>

#### Linked Allocation

![os_12_6]({{ "/assets/OS/OS_12_6.png" | absolute_url }}){: width="400" }{: .center}  
연결 리스트를 사용해 비어 있는 블록에 파일의 데이터를 저장하는 방법이다. 블록에 content와 다음 블록 위치 포인터를 같이 저장한다.  

**Linked Allocation 단점**
1. Direct access 불가능  
파일 임의의 블록을 가고 싶어도 첫번째 블록부터 돌아야 가능하다.  
<br>

2. I/O 속도 느림  
파일 저장 블록이 떨어져 있기 때문에 디스크 헤드가 많이 움직여야 함으로 속도도 떨어진다. 
<br>

3. Reliability(신뢰성) 문제  
블록 베드섹터가 발생하면 연결고리가 끊기기 때문에 다음 블록 접근이 불가능하다.
<br>

4. Pointer로 인한 저장공간 낭비  
컴퓨터와 디스크 인터페이스는 512byte 배수로 데이터를 주고받고 섹터크기도 512byte이다. 때문에 512byte 이하크기의 파일도 512byte 배수크기로 저장된다. 그런데 포인터가 4byte를 차지하기 때문에 섹터의 크기는 508byte크기가 되고 512byte 크기로 전달된 파일을 한 섹터에 저장하지 못하고 두 섹터를 차지하게 되는 공간낭비가 발생한다.
<br>

**Linked Allocation 장점**
1. 외부조각이 발생하지 않는다.  
<br>

단점이 많기 때문에 이대로 사용하지 않고 Linked Allocation을 파일 시스템에 그대로 적용하지 않고 약간 변형해서 사용하는데 그것이 마이크로 소프트의 File-allocation table(FAT) 파일 시스템이다.  
포인터를 별도의 위치에 보관해 Reliability와 공간효율성 문제를 해결한 파일 시스템이다.
<br>
<br>

#### Indexed Allocation

![os_12_7]({{ "/assets/OS/OS_12_7.png" | absolute_url }}){: width="400" }{: .center}  
직접접근을 가능하게 하기 위해서 index블록(19번블록)을 따로 두고 메타데이터에 index블록 저장 위치를 저장하고 있다.  
index블록에는 파일이 저장된 블록 순번대로 블록 위치를 기록해 놓는다. Jeep의 4번째 블록을 보고 싶다면 19번 블록에 접근한 후 네 번째인 10번 블록에 접근하면 된다.  

**Indexed Allocation 단점**
>**Index로 인한 저장공간 낭비**  
아무리 작은 파일이라도 2개의 블록이 필요하다(공간낭비). 반대로 아주 큰 파일의 경우 하나의 index블록으로 모든 block위치를 정장하기 부족하다.  
해결방안으로 linked scheme이나 multilevel index를 사용하면 된다.  
Linked scheme은 인덱스 블록을 여러 개로 구성하고 index블록에 마지막 위치에 또다른 index블록 위치를 기록하는 것이다.  
multilevel index는 인덱스 블록들을 가리키는 index블록을 사용하는 것이다(2단계 페이지 테이블 처럼).  
해결 방안들 역시 index블록으로 인해 공간 낭비가 생긴다.
<br>

**Indexed Allocation 장점**
>외부조각이 발생하지 않고 직접접근이 가능하다.
<br>
<br>

위의 저장 방법이 제각각인 파일 시스템 이론들을 실제 시스템에서 어떻게 사용하는지 알아보자
<br>

***

### Unix 파일 시스템 구조
![os_12_8]({{ "/assets/OS/OS_12_8.png" | absolute_url }})   
지금 보는 파일 시스템은 가장 기본적인 파일 시스템이고 이 파일 시스템이 발전해서 fast file system, ext2, 3, 4 등의 파일시스템으로 발전했다.  

하나의 논리적 디스크(원통)에 파일시스템을 설치하였다. 유닉스의 파일시스템 저장구조는 크게 4가지로 나뉜다. <u>Book block, Super block, Inode list, Data block.</u>  

Unix건 FAT이건 모든 파일시스템에는 Boot block이 가장 맨 앞에 위치한다, bootstrap loader라는 부팅에 필요한 기본 정보가 저장돼 있다.  
Boot block이 알려주는 운영체제 설치 위치(커널)을 찾아서 정상적으로 부팅이 된다.  

Super block은 파일 시스템에 관한 총체적인 정보를 담고 있다.  
Inode list와 data block의 경계선이 어디인지, Data block에서 어디가 빈 블록이고 어디가 실제 사용중인 블록인지 관리한다.  

파일의 메타데이터는 그 파일을 가지고 있는 디렉토리에 저장이 된다고 했는데 실제 파일 시스템에선 <u>디렉토리가 모든 메타데이터를 갖고 있지는 않다.</u>  

UNIX의 경우에선 일부의 메타데이터만 디렉토리가 보관하고 <u>대부분의 메타데이터는 Inode list라는 별도의 위치에 보관하고 있는다.</u> 단 파일의 이름은 디렉토리가 보관한다.  

![os_12_9]({{ "/assets/OS/OS_12_9.png" | absolute_url }})   
그림처럼 디렉토리는 이름만 갖고 Inode번호와 매치시킨 다음 나머지 메타데이터는 모두 inode에 저장되어 있는 형식이다. 

![os_12_10]({{ "/assets/OS/OS_12_10.png" | absolute_url }})   
파일 하나당 Inode가 하나씩 생성되는데 이 Inode가 해당파일의 메타데이터를 의미한다고 보면 된다.  

소유주, 접근권한, 최종 수정시간, 크기 등 여러가지 정보를 갖고 있지만 Disk어느 부분에 위치하는지 위치정보가 제일 중요하다.  
뿔뿔이 흩어져 있는 파일의 위치정보는 Indexed Allocation의 기법을 사용하여 저장하고 관리한다.  
모든 파일의 Inode크기는 같지만 파일 크기는 그렇지 않다. 때문에 파일의 크기가 매우 큰 경우 디스크에 분산 저장되어 있는 블록들의 위치정보를 모두 기록하려면 Indexed Allocation 에서 사용한 방법이 효율적이다.  

위치 정보를 기록하는 Index block(direct blocks, single indirect, double indirect, tiriple indirect)들이 있는데 파일의 크기가 작은 경우 direct block에 있는 포인터 몇 개로만 파일 위치를 가리키고. 대단히 큰 파일의 경우 index block이 이중 삼중으로 되어있는 double, triple indirect블록을 사용해서 해결한다. 이런 index block들은 Data block 어딘가 저장돼있다.  

크기가 작은 파일이 대부분이기 때문에 Indexed Allocation 방법이 효율적이다.
<br>

***

### FAT(File Allocation Table) 파일 시스템
 
![os_12_11]({{ "/assets/OS/OS_12_11.png" | absolute_url }})   
마이크로 소프트가 dos를 만들었을 때 만들어진 파일 시스템. FAT과 Root directory가 생겼다.  

FAT공간에 각 파일들의 <u>메타데이터의 위치정보</u>만 저장되고 실제 메타데이터는 각 디렉토리가 가지고 있다. 뿐만 아니라 해당 파일 첫번째 block(파일 시작 위치정보)이 Data block 어느 위치에 저장되어 있는지도 디렉토리가 보관한다.  

FAT파일 시스템은 <u>Linked Allocation</u>을 사용한다. Linked Allocation단점이 몇 가지 있었는데 이를 해결하기 위해 FAT라는 시스템을 사용한다.  

FAT안에는 Data block안의 block개수만큼의 배열이 생성되고(이 배열이 FAT의 크기를 결정) 각 배열의 index를 block번호로 사용해서 <u>해당 블록이 가리키는 다음 블록의 번호</u>를 저장한다 (EOF는 end of file: 파일의 끝).  

FAT만 확인하면 Linked Allocation의 여러 단점이 다 극복된다. 베드섹터 발생해도 FAT안에 위치정보는 있으니까 건너뛰고 접근 가능하기 때문에 신뢰성 회복되고, 직접접근 되고, 블록에 포인터도 없어도 되니 512byte활용도 가능하다.  

FAT의 정보는 중요하기 때문에 복사본을 여러 개 저장해 놓는다(적어도 2개 이상).  
<br>

***

### Free space management(비어 있는 블록들을 관리하는 방법)
파일시스템이 어떻게 파일의 block들을 관리하는지 보았으니 빈공간은 어떻게 관리하는지 알아보자.  

#### Bit map(Bit vector)

![os_12_12]({{ "/assets/OS/OS_12_12.png" | absolute_url }})   
Data block의 수만큼 bit를 저장할 수 있는 배열을 구성해서 0과 1을 사용해 비어 있는지 파일이 저장되어 있는지 판단하는 방법이다.  

파일이 생기거나 사라질 때 마다 bit map도 수정해야한다. 물론 비어 있는 공간을 찾기 위해선 디스크 헤드가 한바퀴 돌아줘야 한다.  

Bit map의 단점은 배열을 저장할 부가적인 공간이 필요하고(그렇게 크진 않음) 장점은 연속적인 빈 블록을 찾는데 효과적이다. 파일을 저장할 때 가능하면 연속적으로 저장하는 것이 효율적인데 그런 부분에선 bit map이 효과적이다.  
<br>
<br>

#### Linked list
![os_12_13]({{ "/assets/OS/OS_12_13.png" | absolute_url }}){: width="400" }{: .center}  
디스크 헤드가 돌면서(lseek) 비어 있는 블록을 찾고(선수작업) 블록안에 포인터를 두고 연결시켜 놓은 것이다. 공간 낭비는 없어지지만 연속적인 가용공간 찾기도 힘들고 비효율적이다.  
<br>
<br>
 
#### Grouping

![os_12_14]({{ "/assets/OS/OS_12_14.png" | absolute_url }})   
앞에서 배웠던 indexed allocation 방법을 적용한 예이다.  
첫번째 빈 블록이 인덱스 용으로 다른 빈 블록들 위치들을 가리키고, 그중 마지막 블록이 또 다른 빈 블록 위치를 가리키는 방법. 연속적인 빈 블록을 찾기는 마찬가지로 힘들다.  
<br>
<br>

#### Counting

연속적인 빌 블록을 찾기 쉬운 방법.  
(first free block, N of contiguous fee block) 빈 블록 위치와 연속적으로 비어 있는 블록 수를 쌍으로 저장해서 유지 관리하는 방법이다.
<br>

***

### Directory Implementation (디렉토리 구현)
디렉토리는 디렉토리 아래 파일들의 메타데이터를 관리하는 특별한 파일이다, 파일들의 메타데이터를 디렉토리가 어떻게 관리하는지 알아보자.

#### Linear list(순차적 리스트)

![os_12_15]({{ "/assets/OS/OS_12_15.png" | absolute_url }})   
파일의 이름과 파일의 메타데이터들을 순차적으로 저장하는 방법이다.  
고정크기의 배열에 파일 이름과 메타데이터 내용을 저장한다.  
구현은 간다 하지만 파일 찾는 과정도 순차적으로 하나씩 검사해야 하기 때문에 연산에서 많은 시간이 걸린다.

#### Hash Table
![os_12_16]({{ "/assets/OS/OS_12_16.png" | absolute_url }})   
순차 리스트보다 좀더 효율적인 디렉토리 연산을 위한 방법이다.  
파일의 이름을 hash 함수를 적용해서 파일 이름을 저장, 해시 함수란 어떤 input값이 들어오면 특정 숫자로 결과값이 나오도록 하는 함수다. 해시의 결과값을 entry로 파일 이름과 메타데이터를 저장하는 것이다. 물론 해시 함수의 collision이 발생할 수 있다.  
<br>

유닉스의 Inode나 FAT 파일 시스템에선 메타데이터를 디렉토리가 아닌 별도의 공간에 저장하기 때문에 위의 내용과 관계없다. 
<br>

***

### Long file name (긴 파일 이름 )지원
파일 이름을 제한할 수 있지만 비효율적이기 때문에 긴 이름을 지원하기위해 아래 방법을 사용한다.
![os_12_17]({{ "/assets/OS/OS_12_17.png" | absolute_url }})   
만약 파일 이름이 3글자까지라고 할 때 길어서 공간을 벗어날 경우 포인터를 두어 디렉토리 파일 저장공간 뒤쪽부터 나머지 이름을 저장한다.
<br>

***

### VFS and NFS
![os_12_18]({{ "/assets/OS/OS_12_18.png" | absolute_url }})   
#### VFS (Virtual file system)
파일 시스템은 FAT, NTFS, ext2, 3, 4 등 굉장히 다양하다, 사용자가 파일시스템에 접근할 때는 시스템 콜을 통해 접근하는데 파일 시스템 마다 서로 다른 시스템 콜을 사용해야 한다면 사용자 입장에선 혼란스러울 것이다.  
어떤 파일 시스템이 사용되어도 VFS interface라는 OS layer를 거치면 동일한 시스템콜 인터페이스(API)를 통해 접근 가능하다. 

#### NFS (Network File System)
분산 시스템에선 네트워크를 통해 파일이 공유될 수 있다. 로컬의 파일시스템 말고 원격(서버의) 파일시스템에 접근해야 한다. VFS interface를 통해 들어갔는데 로컬에 없을 경우 NFS모듈과 RPC/XDR 원격 접근 프로토콜을 통해 원격 파일 시스템에 접근한다.
<br>

***

### Page cache and Buffer cache
#### Page cache
Paging System의 물리메모리의 page frame들을 page cache라 부른다.  
Backing store에서 page를 불러오는 것보다 물리메모리에서 불러오는 것이 빠르기 때문에 cache라는 개념을 붙여 사용한다.  
<br>

#### Buffer cache
파일의 데이터를 사용자가 요청했을 때 디스크에서 읽어 사용자에게 전달해주고 끝내지 않고 읽어온 내용을 OS 메모리의 특정 영역에 저장해 놓고 다시 해당 내용을 다시 요청할 때 OS에서 읽어오는 것(512byte논리 블록 단위로 진행한다).  

이것도 디스크가 아닌 물리메모리(커널부분)에서 읽어 오기 때문에 cache라는 개념을 붙였다.  

Buffer cache는 OS가 빠른 파일 입출력을 위해 만든 파일 content(block)를 저장하는 공간이다.  
page cache는 프로세스의 빠른 동작을 위해 자주 사용하는 내용을 page단위로 저장해 놓은 공간이다.  

전 챕터에서 배웠던 건데 page는 page fault가 일어나지 않는 이상 OS가 관여하지 못하기 때문에 LRU, LFU를 사용 못하고 clock 알고리즘을 사용했다.  

하지만 buffer cache의 경우 파일의 내용을 집어넣고 프로세스가 요청한 block을 지원해주는 작업들을 모두 OS가 관리하기 때문에 어느 block이 많이, 빈번히 사용되는지 알기 때문에 공간이 부족해지면 LFU나 LRU알고리즘을 통해 쓸모 없는 block을 buffer cache에서 내릴 수 있다.  

***

### 파일 접근 시스템콜(read and write system call, Memory mapped I/O system call)

#### Memory mapped I/O system call
Memory mapped I/O는 파일을 프로세스의 메모리 공간에 매핑 시키고 자신의 메모리 공간에 접근하는 방법이다.  
따라서 한번 매핑시켜 놓으면 파일에 접근할 때 open->read or write를 사용하지 않고 바로 메모리에 접근해서 읽고 수정할 수 있다.
![os_12_19]({{ "/assets/OS/OS_12_19.png" | absolute_url }})   

매핑이라는건 정말 파일 content를 메모리에 올리는 것이 아니라 연결시켜 놓았다는 것이다.  
매핑만 해놓고 만약 파일 content가 아직 메모리에 올라오지 않았다면 page fault가 발생하고 해당 내용을 data block에서 올려야 한다.  

Read, write시스템콜을 통해 파일에 접근할 경우OS가 파일을 읽어 512 byte단위로 buffer cache에 저장해서 프로세스에 지원해주기 때문에 답답한 면이 있다.  
반면에 Memory mapped I/O는 block을 buffer가 아닌 page cache에 올리기 때문에 4KB단위로 저장, 관리한다.  

Memory mapped I/O의 문제는 프로세스들이 동일한 file을 접근해 읽고 수정할 때 생기는 동기화 문제이다.  
buffer cash에선 OS가 원본을 관리하지만 Memory mapped I/O는 OS의 개입이 부족하기 때문에 일관성 문제가 발생할 수 있다.
<br>
<br>

#### Unified Buffer cache (통합 버퍼 캐시)
 
위에서 설명한 파일 접근 방법 두가지는 Unified Buffer cache를 사용여부에 따라 방법이 나뉜다
![os_12_20]({{ "/assets/OS/OS_12_20.png" | absolute_url }})   

최근 OS에선 buffer cache와 page cache개념이 통합되어 관리하는 운영체제가 많다, . 2.4 커널 이전의 시스템에선 사용되지 않았다.  

Unified buffer cache를 사용하지 않는 전통적인 시스템에서 Memory mapped I/O를 하더라도 우선 OS가 buffer cache로 읽고 이걸 다시 사용자 프로세스의 page cache로 전달한다.  

Unified Buffer cache사용하는 시스템에선 read, write시스템콜, Memory mapped I/O 모두 page cache에 접근한다. Buffer cache와 중복 저장이 없고 디바이스 드라이버를 호출해(OS 시스템콜) 바로 메모리에 읽고 쓴다.  

Unified buffer cache에서 Buffer cache사용용도는 파일 접근 입출력용이 아닌 Inode 같은 메타 데이터 관련 블록을 저장하고 관리한다.  
아무래도 민감한 데이터이다 보니 OS에서 적극적으로 관리하는 것 같다. 
(참고자료: https://www.usenix.org/legacy/publications/library/proceedings/usenix2000/freenix/full_papers/silvers/silvers_html/index.html, https://brunch.co.kr/@alden/25)
그래서 최근엔 buffer cache를 그냥 page cache라 하기도 한다.

**추가설명**
>프로세스의 주소공간(virtual memory)는 code, data, stack으로 이루어 진다. data와 stack은 메모리에 올라갔다 사용되지 않는 부분은 swap area로 내려간다.  
>그런데 code는 사용되지 않는 부분이 swap area로 내려가지 않고 아예 사라져 버린다, 어차피 file system에서 code를 가져오기 때문에 메모리에 올라와 있지 않은 필요한 code부분이 있다면 실행파일(file system)에서 메모리로 올린다.  
>위처럼 사용하려면 파일 시스템에 있는 code 일부를 매핑해서 사용해야 하고 이러한 작업은 OS의 loader가 해준다. Memory mapped I/O의 대표적 활용 예이다.

