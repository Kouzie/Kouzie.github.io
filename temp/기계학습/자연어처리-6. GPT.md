## GPT-1

> Generative Pre-Training of a Langauage Model  
> 2018 June OpenAI <https://openai.com/research/language-unsupervised>  
> <https://www.youtube.com/watch?v=FeEmmylAF0o>  

`Langauage Model(LM)` 은 입력된 단어의 다음 단어를 예측하는데 많이 사용하는 모델.  

`GPT-1` 또한 BERT 와 마찬가지로 `[Pre-Traning, Fine Tuning]` 2가지 학습단계로 나누어진다.  

### Pre-Training

대량의 `corpus` 를 기반으로 입력된 단어를 기반으로 다음 단어를 예측하는 문제를 학습한다.  
학습을 통해 우리가 알고있는, 알지못하는 자연어의 특성을 가지는 훌륭한 모델 생성이 가능하다.  

논문에서 `GPT` 는 아래그림 같이 `Transformer Decoder` 가 12개 연결된 모델을 사용한다.  

![1](image/gpt1-2.png)  

`LM` 특성상 방대한 양의 `corpus` 를 `labeling` 없이 `Unsupervised Pre-Training` 한다.  

### Fine Tunning

아래 4가지 task 에 좋은 성능을 보여준다.  
이미 이런 문제를 해결하기 위한 대량의 dataset 이 존재함으로 쉽게 Supervised Learning 을 수행할 수 있다.  

- Natural Langauge Inference-Entailment(자연어 전제가설 추론)
- Question Answering(질의응답)
- Semantic Similarity(문장유사도 분석)
- Classification(분류)

기존 BERT 구조와 다른점은 처리하고자 하는 task 에 따라  
입력 데이터 부분, 마지막 layer 구성이 변경된다는 점이다.  

![1](image/gpt1-1.png)  

## GPT-2

> 2019 Feb OpenAI <https://openai.com/research/better-language-models>  
> <https://www.youtube.com/watch?v=3n6157XNYyw>  
> <https://jalammar.github.io/illustrated-gpt2/>  

`GPT-1` 에서 `[Pre-Traning, Fine Tuning]` 2가지 학습단계로 나누었는데  
`GPT-2` 에선 `Fine Tuning` 단계를 삭제했다.  

`Fine Tuning` 은 학습에 사용되는 비용도 문제이지만 task 별로 별개의 모델을 구성해야한다는 마이너한 문제도 있다.  

`GPT-2` 는 `GPT-1` 에 비해 모델 크기도 10배, `Unsupervised Pre-Training` 학습량도 10배이다.  
그리고 모든 `task` 에 대해 응답할 수 있도록 각 task 별로 학습을 진행해야 한다.  
Raddit 과 같은 서비스에서 Answer Question 값을 가져와 학습에 사용했다고 한다.  

GPT-2 는 입력 단어(문장)과 실행할 task 를 같이 넣어 다양한 task 를 하나의 모델에서 처리할 수 있는 LM 이다.  

![1](image/gpt2-1.png)  

## GPT-3

> 2020 Jul Open AI <https://arxiv.org/abs/2005.14165>
> <https://www.youtube.com/watch?v=p24JUVgDkQk>
> <https://jalammar.github.io/how-gpt3-works-visualizations-animations/>

`GPT-3` 를 `Autoregressive Language Model` 이라 부르는데  
여기서 `Autoregressive` 는 과거의 출력값은 현재의 입력값에 사용하는 의미로  
RNN, Transformer Decoder 가 이에 해당한다.  



### Few shot Learning
GPT-3 에서 Fine-Tunning 대신 채택한 방법이 Few shot Learning  
